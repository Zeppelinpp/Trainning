# 财务分析报告Reward Model训练 - 快速开始

## 一、框架推荐

### 推荐框架组合：PyTorch + Transformers + DeepSpeed

**为什么选择这个组合？**

1. **PyTorch**: 
   - 灵活的动态计算图
   - 强大的分布式训练支持 (DDP)
   - 活跃的社区和丰富的资源

2. **Transformers (HuggingFace)**:
   - 提供大量预训练模型
   - 统一的模型接口
   - 便于模型加载和保存

3. **DeepSpeed** (可选):
   - ZeRO优化，显著降低显存占用
   - 支持超大模型训练
   - 混合精度训练加速

### 其他可选框架

- **Accelerate**: 更简单的分布式训练抽象
- **PyTorch Lightning**: 更高层次的训练框架
- **Megatron-LM**: 适合超大规模模型

## 二、环境安装

### 1. 创建虚拟环境

```bash
# 使用conda
conda create -n reward-model python=3.10
conda activate reward-model

# 或使用venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
```

### 2. 安装依赖

```bash
# 使用uv (推荐，更快)
uv pip install -r requirements.txt

# 或使用pip
pip install -r requirements.txt
```

### 3. 验证安装

```python
import torch
print(f"PyTorch版本: {torch.__version__}")
print(f"CUDA可用: {torch.cuda.is_available()}")
print(f"GPU数量: {torch.cuda.device_count()}")
```

## 三、数据准备

### 1. 数据格式说明

每个训练样本包含：
- `system_prompt`: 系统提示词
- `input_data`: 财务数据（JSON格式）
- `report`: 生成的财务分析报告
- `scores`: 三个维度的评分（0-5分）

### 2. 样例数据

项目已提供5条训练样例（`data/train.jsonl`）：
1. Q3季度盈利能力分析
2. 全年资产效率和偿债能力分析
3. 财务风险评估
4. Q2环比变化分析
5. 投资回报率分析

### 3. 准备自己的数据

```python
import json

data = {
    "system_prompt": "请分析...",
    "input_data": {
        "revenue": 12500000,
        "net_income": 1406250,
        ...
    },
    "report": "# 财务分析报告\n\n...",
    "scores": {
        "accuracy": 4.5,          # 准确度：数据计算正确性
        "professionalism": 4.7,    # 专业度：术语使用、结构完整性
        "depth_of_analysis": 4.3   # 分析深度：洞察力、建议质量
    }
}

# 保存为JSONL
with open('data/train.jsonl', 'a', encoding='utf-8') as f:
    f.write(json.dumps(data, ensure_ascii=False) + '\n')
```

### 4. 数据标注建议

- **准确度**：检查数值计算、财务指标是否正确
- **专业度**：评估专业术语使用、报告结构、语言规范性
- **分析深度**：评估洞察深度、因果分析、建议可行性

建议由3-5位财务分析师独立评分后取平均值。

## 四、训练流程

### 1. 修改配置（可选）

编辑 `config.yaml`:

```yaml
model:
  base_model: "Qwen/Qwen2.5-7B"  # 可换成其他模型
  max_length: 4096

training:
  num_epochs: 3
  batch_size: 4              # 根据显存调整
  learning_rate: 1e-5
```

### 2. 单卡训练

```bash
uv run train.py --config config.yaml
```

### 3. 多卡训练（推荐）

```bash
# 4卡训练
bash run_train.sh

# 或直接使用torchrun
torchrun --nproc_per_node=4 train.py --config config.yaml
```

### 4. 使用DeepSpeed（大模型推荐）

```bash
deepspeed train.py --config config.yaml --deepspeed deepspeed_config.json
```

### 5. 监控训练

训练会自动记录到WandB（需在config.yaml中配置）：

```yaml
wandb:
  enabled: true
  project: "financial-reward-model"
  run_name: "experiment-1"
```

或查看本地日志：
```bash
tail -f logs/training.log
```

## 五、评估模型

### 1. 在测试集上评估

```bash
uv run evaluate.py \
    --checkpoint output/best_model \
    --config config.yaml \
    --test_file data/test.jsonl \
    --output evaluation_results.json
```

### 2. 查看评估结果

```bash
cat evaluation_results.json
```

输出包括：
- 各维度的MSE、MAE
- Pearson和Spearman相关系数
- 综合评估指标

## 六、使用模型

### 1. 命令行推理

```bash
uv run inference.py \
    --checkpoint output/best_model \
    --config config.yaml \
    --input sample_report.json
```

### 2. Python代码调用

```python
from inference import RewardModelInference

# 加载模型
inference = RewardModelInference(
    checkpoint_path="output/best_model",
    config_path="config.yaml"
)

# 评分
result = inference.score_report(
    system_prompt="请生成Q3财务分析报告",
    input_data={
        "revenue": 12500000,
        "net_income": 1406250,
        ...
    },
    report="# 2024年Q3财务分析报告\n\n..."
)

print(f"准确度: {result['scores']['accuracy']:.2f}")
print(f"专业度: {result['scores']['professionalism']:.2f}")
print(f"分析深度: {result['scores']['depth_of_analysis']:.2f}")
print(f"综合评分: {result['overall_score']:.2f}")
print(f"等级: {result['grade']}")
```

## 七、分布式训练详解

### PyTorch DDP原理

每个GPU运行一个独立进程，持有完整模型副本：

```
GPU 0: Model Copy 0 → Batch 0
GPU 1: Model Copy 1 → Batch 1
GPU 2: Model Copy 2 → Batch 2
GPU 3: Model Copy 3 → Batch 3
         ↓
    All-Reduce梯度
         ↓
    同步更新所有模型
```

### 关键参数

```bash
torchrun \
    --nproc_per_node=4 \        # 每个节点的GPU数
    --nnodes=1 \                 # 节点数（机器数）
    --node_rank=0 \              # 当前节点序号
    --master_addr=localhost \    # 主节点地址
    --master_port=29500 \        # 主节点端口
    train.py
```

### 多机训练示例

**机器1 (192.168.1.100, 主节点):**
```bash
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \
    --master_addr=192.168.1.100 --master_port=29500 \
    train.py --config config.yaml
```

**机器2 (192.168.1.101):**
```bash
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 \
    --master_addr=192.168.1.100 --master_port=29500 \
    train.py --config config.yaml
```

### 有效批次大小计算

```
有效batch_size = batch_size × gradient_accumulation_steps × num_gpus

例如：4 × 4 × 4 = 64
```

## 八、性能优化建议

### 1. 显存优化

```yaml
# 方法1: 减小batch_size，增加梯度累积
batch_size: 2
gradient_accumulation_steps: 8

# 方法2: 使用DeepSpeed ZeRO
# 启用ZeRO Stage 2或3

# 方法3: 梯度检查点
model.gradient_checkpointing_enable()
```

### 2. 速度优化

```yaml
# 方法1: 增加数据加载worker
num_workers: 8

# 方法2: 混合精度训练
fp16:
  enabled: true

# 方法3: 编译模型 (PyTorch 2.0+)
model = torch.compile(model)
```

### 3. 显存-速度权衡

| 显存占用 | 训练速度 | 推荐配置 |
|---------|---------|---------|
| 高      | 快      | batch_size=8, grad_accum=2 |
| 中      | 中      | batch_size=4, grad_accum=4 |
| 低      | 慢      | batch_size=2, grad_accum=8 |

## 九、后续应用

### 1. 用于RLHF

```python
# 在PPO训练中使用Reward Model
from transformers import AutoModelForCausalLM

policy_model = AutoModelForCausalLM.from_pretrained("your-llm")
reward_model = RewardModelInference("output/best_model", "config.yaml")

# 生成多个候选报告
candidates = policy_model.generate(...)

# 使用Reward Model评分
scores = [reward_model.score_report(...) for c in candidates]

# 基于分数进行强化学习更新
...
```

### 2. 用于质量评估

```python
# 批量评估生成报告质量
reports = load_generated_reports()

for report in reports:
    score = reward_model.score_report(
        report['system_prompt'],
        report['input_data'],
        report['report']
    )
    
    if score['overall_score'] >= 4.0:
        save_high_quality(report)
```

### 3. 用于数据筛选

```python
# 从大量弱标注数据中筛选高质量样本
weak_labeled_data = load_weak_data()
high_quality_data = []

for item in weak_labeled_data:
    score = reward_model.score_report(...)
    
    if score['overall_score'] >= 4.5:
        high_quality_data.append(item)

# 用高质量数据进行监督微调
```

## 十、常见问题

### Q1: 训练时显存不足怎么办？

```bash
# 方案1: 减小batch_size
batch_size: 2  # 从4改为2

# 方案2: 减小max_length
max_length: 2048  # 从4096改为2048

# 方案3: 使用DeepSpeed ZeRO
deepspeed train.py --deepspeed deepspeed_config.json

# 方案4: 使用更小的基座模型
base_model: "Qwen/Qwen2.5-3B"  # 从7B改为3B
```

### Q2: 训练速度太慢怎么办？

```bash
# 方案1: 使用多卡训练
torchrun --nproc_per_node=4 train.py

# 方案2: 启用混合精度
# 在config.yaml中启用fp16

# 方案3: 增加数据加载worker
num_workers: 8
```

### Q3: 如何判断模型是否收敛？

监控以下指标：
- **验证集loss**: 应持续下降并趋于稳定
- **Pearson相关系数**: 应 > 0.7（预测与真实的相关性）
- **各维度MAE**: 应 < 0.3（约1.5分的误差范围）

### Q4: 数据量不够怎么办？

```python
# 方案1: 数据增强 - 改写报告
from llm import rewrite_report
augmented = rewrite_report(original_report)

# 方案2: 使用强模型生成伪标签
strong_model_scores = gpt4_score(report)

# 方案3: 主动学习 - 优先标注难例
uncertainty = model.predict_uncertainty(unlabeled_data)
prioritized = select_high_uncertainty(unlabeled_data, uncertainty)
```

### Q5: 如何持续改进模型？

```python
# 方案1: 收集线上反馈
user_feedback = collect_feedback()
retrain_model(user_feedback)

# 方案2: 定期人工审核
low_confidence_samples = get_low_confidence()
human_review(low_confidence_samples)

# 方案3: 对比学习
# 收集相同数据的不同报告，标注偏好
preference_data = collect_preferences()
train_with_pairwise_loss(preference_data)
```

## 十一、进阶技巧

### 1. 多任务学习

同时训练评分和质量排序：

```python
class MultiTaskRewardModel(nn.Module):
    def forward(self, ...):
        # 任务1: 绝对评分
        scores = self.score_head(hidden)
        
        # 任务2: 相对排序
        ranking_logits = self.ranking_head(hidden)
        
        loss = score_loss + ranking_loss
        return loss
```

### 2. 对比学习

```python
# 拉近高质量报告的表示，推远低质量报告
anchor = high_quality_report
positive = another_high_quality
negative = low_quality_report

loss = contrastive_loss(anchor, positive, negative)
```

### 3. 不确定性估计

```python
# Monte Carlo Dropout
model.train()  # 保持dropout
predictions = [model(x) for _ in range(20)]
mean = np.mean(predictions)
std = np.std(predictions)  # 不确定性

if std > threshold:
    # 需要人工审核
    human_review(x)
```

## 十二、项目检查清单

训练前确认：
- [ ] 数据格式正确（JSONL）
- [ ] 训练/验证/测试集已准备
- [ ] 配置文件已根据硬件调整
- [ ] GPU可用且驱动正常
- [ ] 有足够的存储空间（至少20GB）

训练中监控：
- [ ] Loss正常下降
- [ ] 没有异常的NaN或Inf
- [ ] GPU利用率 > 80%
- [ ] 定期检查验证集指标

训练后验证：
- [ ] 测试集指标达标
- [ ] 推理速度满足需求
- [ ] 模型文件正确保存
- [ ] 文档和配置已记录

## 联系与支持

如有问题，欢迎提Issue或讨论！

祝训练顺利！🚀

