# å¤šç»´åº¦å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæ•´æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ª**å¤šç»´åº¦è¯„åˆ†å¥–åŠ±æ¨¡å‹**ï¼Œé‡‡ç”¨"é»„é‡‘æ ‡å‡† + å—æ§é™çº§"çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œåœ¨ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ä¸Šè¯„ä¼°è´¢åŠ¡åˆ†ææŠ¥å‘Šè´¨é‡ï¼š

1. **åˆ†ææ·±åº¦ (depth)**: 0-4åˆ†
2. **ä¸“ä¸šåº¦ (professionalism)**: 0-4åˆ†  
3. **æ•°å€¼è®¡ç®—å‡†ç¡®æ€§ (accuracy)**: 0-4åˆ†

ä½¿ç”¨**äº¤å‰ç†µæŸå¤±**è®­ç»ƒå¤šå¤´åˆ†ç±»æ¨¡å‹ï¼Œæ¯ä¸ªç»´åº¦ç‹¬ç«‹è¾“å‡º5ä¸ªæ¡£ä½ï¼ˆ0-4ï¼‰çš„åˆ†ç±»ç»“æœã€‚

## ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

### ä¸ä¼ ç»ŸPairwiseæ–¹æ³•çš„å¯¹æ¯”

| ç‰¹æ€§ | Pairwise (Chosen/Rejected) | å¤šç»´åº¦åˆ†ç±» (æœ¬æ–¹æ¡ˆ) |
|------|---------------------------|-------------------|
| è®­ç»ƒä¿¡å· | ç›¸å¯¹åå¥½ï¼ˆAæ¯”Bå¥½ï¼‰ | ç»å¯¹è´¨é‡ï¼ˆ3åˆ†/4åˆ†ï¼‰ |
| å¯è§£é‡Šæ€§ | ä½ï¼ˆä¸çŸ¥é“å¥½åœ¨å“ªï¼‰ | é«˜ï¼ˆå…·ä½“ç»´åº¦å¾—åˆ†ï¼‰ |
| æ•°æ®åˆ©ç”¨ | åªç”¨å¯¹æ¯”å¯¹ | Chosenå’ŒRejectedéƒ½ç”¨ |
| ç»´åº¦ç»†ç²’åº¦ | æ—  | 3ä¸ªç‹¬ç«‹ç»´åº¦ Ã— 5ä¸ªæ¡£ä½ |
| æŸå¤±å‡½æ•° | Ranking Loss | Cross Entropy Loss |
| åº”ç”¨åœºæ™¯ | æ’åºã€æ¯”è¾ƒ | è¯„åˆ†ã€è¯Šæ–­ã€åé¦ˆ |

### "é»„é‡‘æ ‡å‡† + å—æ§é™çº§"çš„ä¼˜åŠ¿

âœ… **ä¿¡å·çº¯ç²¹**ï¼šå·®å¼‚æ˜¯ç²¾ç¡®æ§åˆ¶çš„ï¼ˆå¦‚åªé™ä½æ·±åº¦ç»´åº¦ï¼‰  
âœ… **è´¨é‡ä¿è¯**ï¼šé»„é‡‘å“åº”ç¡®å®é«˜è´¨é‡ï¼Œç¼ºé™·å“åº”æœ‰æ˜ç¡®ç¼ºé™·  
âœ… **å¤šæ ·æ€§**ï¼š5ç§è´¨é‡ç»´åº¦ Ã— 5ç§é™çº§æ–¹å¼ = å¤šæ ·åŒ–ç»„åˆ  
âœ… **å¯è§£é‡Š**ï¼šå…ƒæ•°æ®è®°å½•è¯¦ç»†çš„è´¨é‡ç»´åº¦å’Œé™çº§ç±»å‹  

## ğŸ”„ å®Œæ•´æµç¨‹

```
ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®ç”Ÿæˆ
â”œâ”€ 1. ç”Ÿæˆåˆ†ææ¡†æ¶ï¼ˆå¯é€‰ï¼Œå¦‚å·²æœ‰åˆ™è·³è¿‡ï¼‰
â”‚   â””â”€ python reward_model/data/synthetic_gen.py
â”‚
â”œâ”€ 2. ç”Ÿæˆå¯¹æ¯”å¯¹æ•°æ®é›†
â”‚   â”œâ”€ é»„é‡‘å“åº”ç”Ÿæˆï¼ˆé«˜è´¨é‡æç¤ºè¯ + ä½æ¸©åº¦0.3ï¼‰
â”‚   â”œâ”€ å—æ§é™çº§ï¼ˆé™çº§æç¤ºè¯ + ä¸­æ¸©åº¦0.5ï¼‰
â”‚   â””â”€ è¾“å‡º: comparison_pairs.jsonl
â”‚
â””â”€ 3. AIè£åˆ¤å¤šç»´åº¦æ‰“åˆ†
    â”œâ”€ ä½¿ç”¨æœ€å¼ºæ¨¡å‹ï¼ˆqwen-maxï¼‰
    â”œâ”€ ä¸‰ç»´åº¦ç‹¬ç«‹è¯„åˆ†ï¼ˆ0-4åˆ†ï¼‰
    â””â”€ è¾“å‡º: comparison_pairs_scored.jsonl

ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®éªŒè¯
â””â”€ 4. éªŒè¯æ•°æ®è´¨é‡
    â”œâ”€ åˆ†æ•°å·®å¼‚ç»Ÿè®¡
    â”œâ”€ æ ‡ç­¾åˆ†å¸ƒåˆ†æ
    â””â”€ é—®é¢˜å¯¹æ¯”å¯¹æ£€æµ‹

ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å‹è®­ç»ƒ
â”œâ”€ 5. åŠ è½½æ•°æ®é›†
â”‚   â”œâ”€ MultiDimRewardDataset
â”‚   â””â”€ åŒæ—¶ä½¿ç”¨Chosenå’ŒRejectedæ ·æœ¬
â”‚
â”œâ”€ 6. è®­ç»ƒå¤šå¤´åˆ†ç±»æ¨¡å‹
â”‚   â”œâ”€ 3ä¸ªç‹¬ç«‹çš„åˆ†ç±»å¤´
â”‚   â”œâ”€ æ¯å¤´è¾“å‡º5ä¸ªç±»åˆ«çš„logits
â”‚   â””â”€ äº¤å‰ç†µæŸå¤±
â”‚
â””â”€ 7. æ¨¡å‹è¯„ä¼°å’Œä¿å­˜
    â”œâ”€ æ•´ä½“å‡†ç¡®ç‡
    â”œâ”€ æ¯ç»´åº¦å‡†ç¡®ç‡å’ŒF1
    â””â”€ æœ€ä½³æ¨¡å‹ä¿å­˜

ç¬¬å››é˜¶æ®µï¼šæ¨¡å‹åº”ç”¨
â””â”€ 8. æ¨ç†å’Œè¯„åˆ†
    â”œâ”€ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    â””â”€ å¯¹æ–°æŠ¥å‘Šè¿›è¡Œå¤šç»´åº¦è¯„åˆ†
```

## ğŸ“¦ æ–‡ä»¶ç»“æ„

```
reward_model/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ synthetic_gen_v2.py              # æ•°æ®ç”Ÿæˆä¸»è„šæœ¬
â”‚   â”œâ”€â”€ validate_multidim_pairs.py        # æ•°æ®éªŒè¯è„šæœ¬
â”‚   â”œâ”€â”€ comparison_pairs.jsonl           # ç”Ÿæˆçš„å¯¹æ¯”å¯¹ï¼ˆæœªæ‰“åˆ†ï¼‰
â”‚   â””â”€â”€ comparison_pairs_scored.jsonl    # AIè£åˆ¤æ‰“åˆ†åçš„æ•°æ®
â”‚
â”œâ”€â”€ model.py                              # æ¨¡å‹å®šä¹‰
â”‚   â””â”€â”€ FinancialRewardModel            # å¤šç»´åº¦åˆ†ç±»æ¨¡å‹
â”‚
â”œâ”€â”€ dataset_multidim.py                   # æ•°æ®é›†ç±»
â”‚   â”œâ”€â”€ MultiDimRewardDataset           # å•æ ·æœ¬æ•°æ®é›†
â”‚   â”œâ”€â”€ PairwiseMultiDimDataset         # æˆå¯¹æ•°æ®é›†
â”‚   â””â”€â”€ get_label_distribution()        # æ ‡ç­¾åˆ†å¸ƒåˆ†æ
â”‚
â”œâ”€â”€ train_multidim.py                     # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ MultiDimRewardTrainer           # è®­ç»ƒå™¨
â”‚
â””â”€â”€ inference.py                          # æ¨ç†è„šæœ¬ï¼ˆå¾…åˆ›å»ºï¼‰
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1ï¼šç”Ÿæˆæ•°æ®

```bash
# ç”Ÿæˆå¯¹æ¯”å¯¹å¹¶è‡ªåŠ¨è°ƒç”¨AIè£åˆ¤æ‰“åˆ†
uv run reward_model/data/synthetic_gen_v2.py
```

é…ç½®è¯´æ˜ï¼ˆåœ¨è„šæœ¬çš„`__main__`ä¸­ä¿®æ”¹ï¼‰ï¼š
- `fields`: è¡Œä¸šåˆ—è¡¨ï¼Œé»˜è®¤5ä¸ªè¡Œä¸š
- `n_pairs_per_field`: æ¯ä¸ªè¡Œä¸šç”Ÿæˆçš„å¯¹æ¯”å¯¹æ•°é‡ï¼Œé»˜è®¤10ä¸ª
- `model_configs`: ç”Ÿæˆæ¨¡å‹é…ç½®ï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹
- `judge_model`: è£åˆ¤æ¨¡å‹ï¼Œæ¨è`qwen-max`

### æ­¥éª¤2ï¼šéªŒè¯æ•°æ®

```bash
# æŸ¥çœ‹æ•°æ®è´¨é‡æŠ¥å‘Š
uv run reward_model/data/validate_multidim_pairs.py \
    reward_model/data/comparison_pairs_scored.jsonl

# åŒæ—¶å¯¼å‡ºè¿‡æ»¤åçš„é«˜è´¨é‡æ•°æ®
uv run reward_model/data/validate_multidim_pairs.py \
    reward_model/data/comparison_pairs_scored.jsonl \
    reward_model/data/filtered_pairs.jsonl
```

éªŒè¯æŠ¥å‘ŠåŒ…å«ï¼š
- âœ… æ•´ä½“å’Œå„ç»´åº¦çš„åˆ†æ•°å·®å¼‚ç»Ÿè®¡
- âœ… æ ‡ç­¾åˆ†å¸ƒåˆ†æï¼ˆChosen vs Rejectedï¼‰
- âœ… é—®é¢˜å¯¹æ¯”å¯¹æ£€æµ‹
- âœ… æ”¹è¿›å»ºè®®

### æ­¥éª¤3ï¼šæŸ¥çœ‹æ•°æ®åˆ†å¸ƒ

```bash
# åˆ†ææ ‡ç­¾åˆ†å¸ƒ
uv run python reward_model/dataset_multidim.py \
    reward_model/data/comparison_pairs_scored.jsonl
```

### æ­¥éª¤4ï¼šè®­ç»ƒæ¨¡å‹

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
uv run reward_model/train_multidim.py
```

å…³é”®è®­ç»ƒå‚æ•°ï¼ˆåœ¨`train_multidim.py`çš„`config`ä¸­ä¿®æ”¹ï¼‰ï¼š
```python
config = {
    "base_model_name": "hfl/chinese-roberta-wwm-ext",  # åŸºç¡€æ¨¡å‹
    "max_length": 2048,           # æœ€å¤§åºåˆ—é•¿åº¦
    "batch_size": 4,              # æ‰¹æ¬¡å¤§å°
    "learning_rate": 2e-5,        # å­¦ä¹ ç‡
    "num_epochs": 10,             # è®­ç»ƒè½®æ•°
    "num_dimensions": 3,          # ç»´åº¦æ•°
    "num_classes": 5,             # ç±»åˆ«æ•°ï¼ˆ0-4ï¼‰
    "use_multi_head": True,       # ä½¿ç”¨å¤šå¤´ï¼ˆæ¨èï¼‰
}
```

è®­ç»ƒè¾“å‡ºï¼š
- æ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
- æ•´ä½“å‡†ç¡®ç‡å’Œå„ç»´åº¦å‡†ç¡®ç‡
- å„ç»´åº¦çš„F1åˆ†æ•°
- æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨`outputs/multidim/best_model.pt`

## ğŸ“Š æ•°æ®æ ¼å¼

### è¾“å…¥æ ¼å¼ï¼ˆcomparison_pairs_scored.jsonlï¼‰

```json
{
  "prompt": "è¯·æ ¹æ®ä»¥ä¸‹åˆ†ææ¡†æ¶ç”Ÿæˆåˆ¶é€ ä¸šçš„è´¢åŠ¡åˆ†ææŠ¥å‘Šï¼š...",
  "chosen": "é»„é‡‘å“åº”å…¨æ–‡...",
  "rejected": "ç¼ºé™·å“åº”å…¨æ–‡...",
  "metadata": {
    "field": "åˆ¶é€ ä¸š",
    "model": "qwen-plus",
    "gold_metadata": {
      "quality_focus": "æ•°æ®å‡†ç¡®æ€§å¯¼å‘"
    },
    "defect_metadata": {
      "degradation_type": "æµ…åŒ–æ·±åº¦"
    }
  },
  "scores": {
    "chosen": {
      "depth": 4,
      "professionalism": 3,
      "accuracy": 4
    },
    "rejected": {
      "depth": 1,
      "professionalism": 2,
      "accuracy": 3
    },
    "reasoning": {
      "depth": "é»„é‡‘å“åº”æœ‰æ·±å…¥çš„å½’å› åˆ†æï¼Œç¼ºé™·å“åº”åªæœ‰è¡¨é¢æè¿°",
      "professionalism": "é»„é‡‘å“åº”ä½¿ç”¨äº†ä¸“ä¸šæœ¯è¯­ï¼Œç¼ºé™·å“åº”è¾ƒé€šç”¨",
      "accuracy": "é»„é‡‘å“åº”è®¡ç®—ç²¾ç¡®ï¼Œç¼ºé™·å“åº”æœ‰ç®€åŒ–"
    }
  }
}
```

### æ¨¡å‹è¾“å‡ºæ ¼å¼

```python
outputs = model(input_ids, attention_mask, labels)

# outputsåŒ…å«:
{
    "logits": torch.Tensor,           # [batch, 3, 5] - æ¯ç»´åº¦5ä¸ªç±»åˆ«çš„logits
    "predicted_labels": torch.Tensor, # [batch, 3] - é¢„æµ‹çš„ç±»åˆ«ï¼ˆ0-4ï¼‰
    "loss": torch.Tensor,            # äº¤å‰ç†µæŸå¤±
    "accuracy": float,               # æ•´ä½“å‡†ç¡®ç‡
    "per_dim_accuracy": torch.Tensor # [3] - å„ç»´åº¦å‡†ç¡®ç‡
}
```

## ğŸ“ æ¨¡å‹æ¶æ„

```
FinancialRewardModel
â”œâ”€â”€ Base Model (e.g., RoBERTa)
â”‚   â””â”€â”€ Hidden States [batch, seq_len, hidden_size]
â”‚
â”œâ”€â”€ Pooling (Last Token / Mean / CLS)
â”‚   â””â”€â”€ Pooled Output [batch, hidden_size]
â”‚
â””â”€â”€ Multi-Head Classification (æ¨è)
    â”œâ”€â”€ Head 1 (Depth)
    â”‚   â”œâ”€â”€ Linear(hidden_size, hidden_size//2)
    â”‚   â”œâ”€â”€ ReLU + Dropout
    â”‚   â””â”€â”€ Linear(hidden_size//2, 5) â†’ [batch, 5]
    â”‚
    â”œâ”€â”€ Head 2 (Professionalism)
    â”‚   â””â”€â”€ ... â†’ [batch, 5]
    â”‚
    â””â”€â”€ Head 3 (Accuracy)
        â””â”€â”€ ... â†’ [batch, 5]
    
    Final Output: [batch, 3, 5]
```

### æŸå¤±å‡½æ•°

```python
# å¯¹æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªç»´åº¦è®¡ç®—äº¤å‰ç†µ
loss = CrossEntropyLoss(
    input=logits.view(-1, 5),      # [batch*3, 5]
    target=labels.view(-1)          # [batch*3]
)

# ç›¸æ¯”MSEå›å½’çš„ä¼˜åŠ¿ï¼š
# 1. æ˜ç¡®çš„ç±»åˆ«è¾¹ç•Œï¼ˆ0,1,2,3,4ï¼‰
# 2. ä¸å—æ•°å€¼å¤§å°å½±å“
# 3. æ›´é€‚åˆç¦»æ•£è¯„åˆ†ä»»åŠ¡
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### è®­ç»ƒè¿‡ç¨‹ä¸­

1. **æ•´ä½“å‡†ç¡®ç‡**: æ‰€æœ‰ç»´åº¦é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹
2. **å„ç»´åº¦å‡†ç¡®ç‡**: æ¯ä¸ªç»´åº¦ç‹¬ç«‹çš„å‡†ç¡®ç‡
3. **å„ç»´åº¦F1åˆ†æ•°**: å®å¹³å‡F1ï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰

### éªŒè¯æ•°æ®è´¨é‡

1. **å¹³å‡åˆ†å·®**: Chosenå’ŒRejectedçš„å¹³å‡åˆ†å·®
   - ä¼˜ç§€: â‰¥1.5åˆ†
   - è‰¯å¥½: â‰¥1.0åˆ†
   - ä¸€èˆ¬: â‰¥0.5åˆ†

2. **æ­£å·®å¼‚ç‡**: Chosenåˆ†æ•° > Rejectedåˆ†æ•°çš„æ¯”ä¾‹
   - ä¼˜ç§€: â‰¥95%
   - è‰¯å¥½: â‰¥90%
   - ä¸€èˆ¬: â‰¥85%

3. **å„ç»´åº¦åˆ†å·®**: æ¯ä¸ªç»´åº¦ç‹¬ç«‹çš„åˆ†æ•°å·®å¼‚
   - åº”è¯¥ä¸é™çº§ç±»å‹å¯¹åº”ï¼ˆå¦‚é™çº§"æ·±åº¦"æ—¶ï¼Œdepthç»´åº¦å·®å¼‚åº”æœ€å¤§ï¼‰

## ğŸ”§ é«˜çº§é…ç½®

### 1. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡

å¦‚æœæŸäº›åˆ†æ•°æ¡£ä½æ ·æœ¬è¿‡å°‘ï¼Œå¯ä»¥ä½¿ç”¨ç±»åˆ«æƒé‡ï¼š

```python
# åœ¨model.pyçš„forwardä¸­
class_weights = torch.tensor([2.0, 1.5, 1.0, 1.0, 1.0]).to(device)
loss = nn.functional.cross_entropy(
    logits_flat, 
    labels_flat,
    weight=class_weights
)
```

### 2. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹

```python
config = {
    "base_model_name": "THUDM/chatglm3-6b-base",  # æˆ–å…¶ä»–å¤§æ¨¡å‹
    ...
}
```

### 3. å¢åŠ æ•°æ®å¤šæ ·æ€§

ä¿®æ”¹`synthetic_gen_v2.py`ä¸­çš„ç”Ÿæˆæ•°é‡ï¼š

```python
generate_comparison_dataset(
    fields=["åˆ¶é€ ä¸š", "æœåŠ¡ä¸š", "é‡‘èä¸š", "æˆ¿åœ°äº§", "ç§‘æŠ€ä¸š"],
    n_pairs_per_field=50,  # å¢åŠ åˆ°50
    ...
)
```

### 4. è°ƒæ•´è¯„åˆ†æ ‡å‡†

å¦‚æœéœ€è¦æ›´ç»†ç²’åº¦çš„è¯„åˆ†ï¼ˆå¦‚0-9åˆ†10æ¡£ï¼‰ï¼Œä¿®æ”¹ï¼š

```python
# model.py
model = FinancialRewardModel(
    num_classes=10,  # æ”¹ä¸º10æ¡£
    ...
)

# synthetic_gen_v2.pyä¸­çš„AIè£åˆ¤æç¤ºè¯
# å°†0-4æ”¹ä¸º0-9ï¼Œå¹¶é‡æ–°å®šä¹‰æ¯ä¸ªæ¡£ä½çš„æ ‡å‡†
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å°`batch_size`ï¼ˆå¦‚ä»4æ”¹ä¸º2æˆ–1ï¼‰
- å‡å°`max_length`ï¼ˆå¦‚ä»2048æ”¹ä¸º1024ï¼‰
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼š
```python
accumulation_steps = 4
for i, batch in enumerate(train_loader):
    loss = outputs["loss"] / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### Q2: æŸä¸ªç»´åº¦å‡†ç¡®ç‡å¾ˆä½

**å¯èƒ½åŸå› **ï¼š
1. è¯¥ç»´åº¦æ ‡ç­¾åˆ†å¸ƒæåº¦ä¸å¹³è¡¡
2. é™çº§ç­–ç•¥å¯¹è¯¥ç»´åº¦å½±å“ä¸æ˜æ˜¾
3. åŸºç¡€æ¨¡å‹èƒ½åŠ›ä¸è¶³ä»¥åŒºåˆ†è¯¥ç»´åº¦

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨ç±»åˆ«æƒé‡
- é‡æ–°è®¾è®¡è¯¥ç»´åº¦çš„é™çº§ç­–ç•¥
- å¢åŠ è¯¥ç»´åº¦çš„è®­ç»ƒæ ·æœ¬

### Q3: Chosenå’ŒRejectedåˆ†æ•°å·®å¼‚å°

**åŸå› **ï¼šé™çº§ä¸å¤Ÿæ˜æ˜¾

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥é™çº§æç¤ºè¯æ˜¯å¦è¶³å¤Ÿå¼º
- å¢åŠ é™çº§æ¸©åº¦ï¼ˆä»0.5æé«˜åˆ°0.7ï¼‰
- äººå·¥æŠ½æŸ¥å‡ ä¸ªæ ·æœ¬ï¼Œçœ‹é™çº§æ˜¯å¦ç”Ÿæ•ˆ

### Q4: ç”Ÿæˆé€Ÿåº¦æ…¢/è´¹ç”¨é«˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹ï¼ˆå¦‚`qwen-turbo`ï¼‰
- å‡å°‘ç”Ÿæˆæ•°é‡
- ä½¿ç”¨å¼‚æ­¥å¹¶å‘ç”Ÿæˆï¼ˆä¿®æ”¹ä»£ç æ”¯æŒï¼‰

## ğŸ“š å‚è€ƒèµ„æ–™

### å­¦æœ¯è®ºæ–‡

- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155)
- [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)

### ç›¸å…³é¡¹ç›®

- [trl (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)
- [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPRï¼

## ğŸ“„ è®¸å¯

MIT License

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä»£ç æ³¨é‡Šæˆ–æIssueã€‚

