"""
éªŒè¯å¤šç»´åº¦è¯„åˆ†å¯¹æ¯”å¯¹æ•°æ®é›†çš„è´¨é‡
"""

import json
from collections import Counter
from typing import List, Dict, Any
import statistics


def load_pairs(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½å¯¹æ¯”å¯¹æ•°æ®"""
    pairs = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            pairs.append(json.loads(line))
    return pairs


def validate_multidim_scores(pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
    """éªŒè¯å¤šç»´åº¦åˆ†æ•°å·®å¼‚"""
    pairs_with_scores = [p for p in pairs if "scores" in p and "chosen" in p["scores"]]

    if not pairs_with_scores:
        return {
            "error": "æ²¡æœ‰æ‰¾åˆ°æ‰“åˆ†æ•°æ®",
            "total_pairs": len(pairs),
            "scored_pairs": 0,
        }

    # Get dimensions from first pair
    dimensions = list(pairs_with_scores[0]["scores"]["chosen"].keys())
    
    # Calculate per-dimension differences
    dim_diffs = {dim: [] for dim in dimensions}
    avg_diffs = []

    for p in pairs_with_scores:
        for dim in dimensions:
            chosen_score = p["scores"]["chosen"][dim]
            rejected_score = p["scores"]["rejected"][dim]
            dim_diffs[dim].append(chosen_score - rejected_score)

        # Calculate average difference across all dimensions
        chosen_avg = sum(p["scores"]["chosen"].values()) / len(p["scores"]["chosen"])
        rejected_avg = sum(p["scores"]["rejected"].values()) / len(
            p["scores"]["rejected"]
        )
        avg_diffs.append(chosen_avg - rejected_avg)

    positive_diffs = [d for d in avg_diffs if d > 0]
    negative_diffs = [d for d in avg_diffs if d < 0]
    zero_diffs = [d for d in avg_diffs if d == 0]

    result = {
        "total_pairs": len(pairs),
        "scored_pairs": len(pairs_with_scores),
        "coverage": f"{len(pairs_with_scores) / len(pairs) * 100:.1f}%",
        "average_diff": statistics.mean(avg_diffs),
        "median_diff": statistics.median(avg_diffs),
        "min_diff": min(avg_diffs),
        "max_diff": max(avg_diffs),
        "std_dev": statistics.stdev(avg_diffs) if len(avg_diffs) > 1 else 0,
        "positive_diffs": len(positive_diffs),
        "negative_diffs": len(negative_diffs),
        "zero_diffs": len(zero_diffs),
        "quality_ratio": f"{len(positive_diffs) / len(avg_diffs) * 100:.1f}%",
        "dimensions": {},
    }

    # Add per-dimension statistics
    for dim in dimensions:
        diffs = dim_diffs[dim]
        result["dimensions"][dim] = {
            "average_diff": statistics.mean(diffs),
            "median_diff": statistics.median(diffs),
            "min_diff": min(diffs),
            "max_diff": max(diffs),
            "positive_rate": f"{len([d for d in diffs if d > 0]) / len(diffs) * 100:.1f}%",
        }

    return result


def analyze_label_distribution(pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆ†ææ ‡ç­¾åˆ†å¸ƒ"""
    pairs_with_scores = [p for p in pairs if "scores" in p and "chosen" in p["scores"]]

    if not pairs_with_scores:
        return {"error": "æ²¡æœ‰æ‰¾åˆ°æ‰“åˆ†æ•°æ®"}

    dimensions = list(pairs_with_scores[0]["scores"]["chosen"].keys())

    # Count labels
    chosen_counts = {dim: Counter() for dim in dimensions}
    rejected_counts = {dim: Counter() for dim in dimensions}

    for p in pairs_with_scores:
        for dim in dimensions:
            chosen_counts[dim][p["scores"]["chosen"][dim]] += 1
            rejected_counts[dim][p["scores"]["rejected"][dim]] += 1

    # Calculate distribution
    total = len(pairs_with_scores)
    result = {
        "total_pairs": total,
        "dimensions": {},
    }

    for dim in dimensions:
        result["dimensions"][dim] = {
            "chosen": {
                label: count / total for label, count in chosen_counts[dim].items()
            },
            "rejected": {
                label: count / total for label, count in rejected_counts[dim].items()
            },
        }

    return result


def find_problematic_pairs(
    pairs: List[Dict[str, Any]], min_avg_diff: float = 0.5
) -> List[Dict[str, Any]]:
    """æ‰¾å‡ºæœ‰é—®é¢˜çš„å¯¹æ¯”å¯¹"""
    problematic = []

    for i, p in enumerate(pairs):
        if "scores" not in p:
            problematic.append({
                "index": i,
                "reason": "ç¼ºå°‘è¯„åˆ†",
                "metadata": p.get("metadata", {}),
            })
            continue

        if "chosen" not in p["scores"]:
            problematic.append({
                "index": i,
                "reason": "è¯„åˆ†æ ¼å¼é”™è¯¯",
                "metadata": p.get("metadata", {}),
            })
            continue

        # Calculate average difference
        chosen_avg = sum(p["scores"]["chosen"].values()) / len(p["scores"]["chosen"])
        rejected_avg = sum(p["scores"]["rejected"].values()) / len(
            p["scores"]["rejected"]
        )
        avg_diff = chosen_avg - rejected_avg

        if avg_diff < min_avg_diff:
            problematic.append({
                "index": i,
                "reason": f"å¹³å‡åˆ†å·®å¼‚è¿‡å° ({avg_diff:.2f} < {min_avg_diff})",
                "chosen_avg": chosen_avg,
                "rejected_avg": rejected_avg,
                "chosen_scores": p["scores"]["chosen"],
                "rejected_scores": p["scores"]["rejected"],
                "metadata": p.get("metadata", {}),
            })

    return problematic


def print_report(pairs: List[Dict[str, Any]]):
    """æ‰“å°éªŒè¯æŠ¥å‘Š"""
    print("=" * 70)
    print("å¤šç»´åº¦å¯¹æ¯”å¯¹æ•°æ®é›†éªŒè¯æŠ¥å‘Š")
    print("=" * 70)

    # 1. Multi-dimensional score differences
    print("\nğŸ“Š 1. å¤šç»´åº¦åˆ†æ•°å·®å¼‚ç»Ÿè®¡")
    print("-" * 70)
    score_stats = validate_multidim_scores(pairs)
    
    if "error" not in score_stats:
        print(f"æ€»å¯¹æ¯”å¯¹æ•°: {score_stats['total_pairs']}")
        print(f"å·²è¯„åˆ†æ•°: {score_stats['scored_pairs']} ({score_stats['coverage']})")
        print(f"\næ•´ä½“å¹³å‡åˆ†å·®: {score_stats['average_diff']:.2f}")
        print(f"æ•´ä½“ä¸­ä½åˆ†å·®: {score_stats['median_diff']:.2f}")
        print(f"åˆ†å·®èŒƒå›´: [{score_stats['min_diff']:.2f}, {score_stats['max_diff']:.2f}]")
        print(f"æ ‡å‡†å·®: {score_stats['std_dev']:.2f}")
        print(f"\nåˆ†å·®åˆ†å¸ƒ:")
        print(f"  æ­£å·®å¼‚ (chosen > rejected): {score_stats['positive_diffs']} ({score_stats['quality_ratio']})")
        print(f"  è´Ÿå·®å¼‚ (chosen < rejected): {score_stats['negative_diffs']}")
        print(f"  é›¶å·®å¼‚ (chosen = rejected): {score_stats['zero_diffs']}")

        # Per-dimension statistics
        dimension_names = {
            "depth": "åˆ†ææ·±åº¦",
            "professionalism": "ä¸“ä¸šåº¦",
            "accuracy": "æ•°å€¼å‡†ç¡®æ€§",
        }

        print(f"\nå„ç»´åº¦åˆ†å·®ç»Ÿè®¡:")
        for dim, stats in score_stats["dimensions"].items():
            dim_name = dimension_names.get(dim, dim)
            print(f"\n  {dim_name} ({dim}):")
            print(f"    å¹³å‡åˆ†å·®: {stats['average_diff']:.2f}")
            print(f"    ä¸­ä½åˆ†å·®: {stats['median_diff']:.2f}")
            print(f"    åˆ†å·®èŒƒå›´: [{stats['min_diff']}, {stats['max_diff']}]")
            print(f"    æ­£å·®å¼‚ç‡: {stats['positive_rate']}")

        # Quality assessment
        avg_diff = score_stats["average_diff"]
        quality_ratio = float(score_stats["quality_ratio"].rstrip("%"))

        print(f"\nâœ… è´¨é‡è¯„ä¼°:")
        if avg_diff >= 1.5 and quality_ratio >= 95:
            print("  ğŸŒŸ ä¼˜ç§€ - å¯¹æ¯”å¯¹è´¨é‡éå¸¸å¥½ï¼Œchosenæ˜¾è‘—ä¼˜äºrejected")
        elif avg_diff >= 1.0 and quality_ratio >= 90:
            print("  âœ… è‰¯å¥½ - å¯¹æ¯”å¯¹è´¨é‡ç¬¦åˆé¢„æœŸ")
        elif avg_diff >= 0.5 and quality_ratio >= 85:
            print("  âš ï¸  ä¸€èˆ¬ - å¯¹æ¯”å¯¹è´¨é‡å¯æ¥å—ï¼Œä½†å»ºè®®ä¼˜åŒ–")
        else:
            print("  âŒ è¾ƒå·® - å¯¹æ¯”å¯¹è´¨é‡éœ€è¦æ”¹è¿›")
    else:
        print(f"âŒ {score_stats['error']}")

    # 2. Label distribution
    print("\nğŸ“ˆ 2. æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡")
    print("-" * 70)
    dist_stats = analyze_label_distribution(pairs)

    if "error" not in dist_stats:
        dimension_names = {
            "depth": "åˆ†ææ·±åº¦",
            "professionalism": "ä¸“ä¸šåº¦",
            "accuracy": "æ•°å€¼å‡†ç¡®æ€§",
        }

        for dim, stats in dist_stats["dimensions"].items():
            dim_name = dimension_names.get(dim, dim)
            print(f"\n{dim_name} ({dim}):")

            print("  Chosenæ ·æœ¬åˆ†å¸ƒ:")
            for label in sorted(stats["chosen"].keys()):
                pct = stats["chosen"][label] * 100
                bar = "â–ˆ" * int(pct / 2)
                print(f"    {label}åˆ†: {pct:5.1f}% {bar}")

            print("  Rejectedæ ·æœ¬åˆ†å¸ƒ:")
            for label in sorted(stats["rejected"].keys()):
                pct = stats["rejected"][label] * 100
                bar = "â–ˆ" * int(pct / 2)
                print(f"    {label}åˆ†: {pct:5.1f}% {bar}")
    else:
        print(f"âŒ {dist_stats['error']}")

    # 3. Problematic pairs
    print("\nâš ï¸  3. é—®é¢˜å¯¹æ¯”å¯¹æ£€æµ‹")
    print("-" * 70)
    problematic = find_problematic_pairs(pairs, min_avg_diff=0.5)

    if problematic:
        print(f"å‘ç° {len(problematic)} ä¸ªéœ€è¦å…³æ³¨çš„å¯¹æ¯”å¯¹:")
        for p in problematic[:5]:  # Only show first 5
            print(f"\n  ç´¢å¼• {p['index']}:")
            print(f"    åŸå› : {p['reason']}")
            if "chosen_scores" in p:
                print(f"    Chosenåˆ†æ•°: {p['chosen_scores']}")
                print(f"    Rejectedåˆ†æ•°: {p['rejected_scores']}")
        if len(problematic) > 5:
            print(f"\n  ... è¿˜æœ‰ {len(problematic) - 5} ä¸ªé—®é¢˜å¯¹æ¯”å¯¹")

        print(f"\n  é—®é¢˜å æ¯”: {len(problematic) / len(pairs) * 100:.1f}%")
    else:
        print("âœ… æ‰€æœ‰å¯¹æ¯”å¯¹è´¨é‡è‰¯å¥½")

    # 4. Recommendations
    print("\nğŸ’¡ 4. æ”¹è¿›å»ºè®®")
    print("-" * 70)
    recommendations = []

    if "error" not in score_stats:
        if score_stats["average_diff"] < 1.0:
            recommendations.append("â€¢ æ•´ä½“åˆ†æ•°å·®å¼‚åå°ï¼Œå»ºè®®å¢å¼ºé™çº§æç¤ºè¯çš„å¼ºåº¦")

        # Check per-dimension differences
        for dim, stats in score_stats["dimensions"].items():
            if stats["average_diff"] < 0.5:
                dim_name = dimension_names.get(dim, dim)
                recommendations.append(
                    f"â€¢ {dim_name}ç»´åº¦å·®å¼‚è¿‡å° ({stats['average_diff']:.2f})ï¼Œå»ºè®®é’ˆå¯¹æ€§ä¼˜åŒ–é™çº§ç­–ç•¥"
                )

        if float(score_stats["quality_ratio"].rstrip("%")) < 90:
            recommendations.append("â€¢ å­˜åœ¨è¾ƒå¤šè´Ÿå·®å¼‚ï¼Œå»ºè®®æ£€æŸ¥é»„é‡‘å“åº”å’Œé™çº§é€»è¾‘")

        if score_stats["negative_diffs"] > 0:
            recommendations.append(
                f"â€¢ å‘ç° {score_stats['negative_diffs']} ä¸ªè´Ÿå·®å¼‚å¯¹æ¯”å¯¹ï¼Œå»ºè®®äººå·¥æ£€æŸ¥æˆ–è¿‡æ»¤"
            )

    if problematic:
        problem_ratio = len(problematic) / len(pairs)
        if problem_ratio > 0.1:
            recommendations.append(
                f"â€¢ é—®é¢˜å¯¹æ¯”å¯¹å æ¯” {problem_ratio * 100:.1f}%ï¼Œå»ºè®®è¿‡æ»¤æˆ–é‡æ–°ç”Ÿæˆ"
            )

    # Check label distribution balance
    if "error" not in dist_stats:
        for dim, stats in dist_stats["dimensions"].items():
            if stats["chosen"]:
                max_pct = max(stats["chosen"].values())
                min_pct = min(stats["chosen"].values())
                if max_pct > min_pct * 3:
                    dim_name = dimension_names.get(dim, dim)
                    recommendations.append(
                        f"â€¢ {dim_name}ç»´åº¦æ ‡ç­¾åˆ†å¸ƒä¸å‡è¡¡ï¼Œå»ºè®®è°ƒæ•´ç”Ÿæˆç­–ç•¥æˆ–ä½¿ç”¨ç±»åˆ«æƒé‡"
                    )

    if recommendations:
        for rec in recommendations:
            print(rec)
    else:
        print("âœ… æ•°æ®é›†è´¨é‡è‰¯å¥½ï¼Œæ— éœ€æ”¹è¿›")

    print("\n" + "=" * 70)


def export_filtered_dataset(
    input_file: str,
    output_file: str,
    min_avg_diff: float = 0.5,
    require_positive_diff: bool = True,
):
    """å¯¼å‡ºè¿‡æ»¤åçš„é«˜è´¨é‡æ•°æ®é›†"""
    pairs = load_pairs(input_file)

    filtered = []
    for p in pairs:
        if "scores" not in p or "chosen" not in p["scores"]:
            continue

        # Calculate average difference
        chosen_avg = sum(p["scores"]["chosen"].values()) / len(p["scores"]["chosen"])
        rejected_avg = sum(p["scores"]["rejected"].values()) / len(
            p["scores"]["rejected"]
        )
        avg_diff = chosen_avg - rejected_avg

        if require_positive_diff and avg_diff <= 0:
            continue

        if avg_diff < min_avg_diff:
            continue

        filtered.append(p)

    with open(output_file, "w", encoding="utf-8") as f:
        for p in filtered:
            f.write(json.dumps(p, ensure_ascii=False) + "\n")

    print(f"\nå·²å¯¼å‡º {len(filtered)}/{len(pairs)} ä¸ªé«˜è´¨é‡å¯¹æ¯”å¯¹åˆ° {output_file}")
    print(f"è¿‡æ»¤ç‡: {(1 - len(filtered) / len(pairs)) * 100:.1f}%")


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python validate_multidim_pairs.py <input_file> [output_file]")
        print("ç¤ºä¾‹: python validate_multidim_pairs.py comparison_pairs_scored.jsonl")
        print(
            "      python validate_multidim_pairs.py comparison_pairs_scored.jsonl filtered_pairs.jsonl"
        )
        sys.exit(1)

    input_file = sys.argv[1]
    pairs = load_pairs(input_file)

    # Print validation report
    print_report(pairs)

    # Export filtered dataset if output file specified
    if len(sys.argv) >= 3:
        output_file = sys.argv[2]
        print("\n" + "=" * 70)
        print("å¯¼å‡ºè¿‡æ»¤åçš„æ•°æ®é›†")
        print("=" * 70)
        export_filtered_dataset(input_file, output_file, min_avg_diff=0.5)

